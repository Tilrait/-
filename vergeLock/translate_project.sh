#!/bin/bash
# –£–ø—Ä–∞–≤–ª—è—é—â–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –ø—Ä–æ–µ–∫—Ç–∞ VergeLock

set -e

# –¶–≤–µ—Ç–∞ –¥–ª—è –≤—ã–≤–æ–¥–∞
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –≤—ã–≤–æ–¥–∞
log_info() { echo -e "${GREEN}[INFO]${NC} $1"; }
log_warn() { echo -e "${YELLOW}[WARN]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
check_dependencies() {
    log_info "–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π..."
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º Python
    if ! command -v python3 &> /dev/null; then
        log_error "Python3 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"
        exit 1
    fi
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º pip
    if ! command -v pip &> /dev/null; then
        log_warn "Pip –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º..."
        python3 -m ensurepip --upgrade
    fi
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
    python3 -c "import requests, markdown, dotenv" 2>/dev/null || {
        log_warn "–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏..."
        pip install requests markdown python-dotenv beautifulsoup4
    }
    
    log_info "‚úÖ –í—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã"
}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ API –∫–ª—é—á–∞
setup_api_key() {
    if [ ! -f .env ]; then
        log_info "–ù–∞—Å—Ç—Ä–æ–π–∫–∞ API –∫–ª—é—á–∞ DeepSeek..."
        echo "–í–≤–µ–¥–∏—Ç–µ –≤–∞—à API –∫–ª—é—á DeepSeek:"
        read -s api_key
        
        echo "DEEPSEEK_API_KEY=$api_key" > .env
        log_info "‚úÖ API –∫–ª—é—á —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ .env"
    else
        log_info "‚úÖ API –∫–ª—é—á —É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω"
    fi
}

# –°–æ–∑–¥–∞–Ω–∏–µ –≥–ª–æ—Å—Å–∞—Ä–∏—è
create_glossary() {
    local input_file=$1
    local glossary_file="glossary.json"
    
    if [ -f "$glossary_file" ]; then
        log_info "–ì–ª–æ—Å—Å–∞—Ä–∏–π —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"
        return
    fi
    
    log_info "–°–æ–∑–¥–∞–Ω–∏–µ –≥–ª–æ—Å—Å–∞—Ä–∏—è –∏–∑ $input_file..."
    
    cat > extract_terms.py << 'PYEOF'
import re
import json
import sys
from collections import Counter

def extract_terms(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç–µ—Ä–º–∏–Ω–æ–≤
    patterns = [
        r'\b[–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+\b',
        r'\b[–ê-–Ø–Å][–∞-—è—ë]{4,}\b',
        r'\b[A-Z][a-z]+\s+[A-Z][a-z]+\b',
        r'\b[a-zA-Z]{6,}\b',
    ]
    
    terms = []
    for pattern in patterns:
        terms.extend(re.findall(pattern, content))
    
    # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—É
    counter = Counter(terms)
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º
    common = {'–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä—ã–µ', '—Ç–∞–∫–æ–π', '—Ç–∞–∫–∏–µ', '–º–æ–∂–µ—Ç', '–¥–æ–ª–∂–µ–Ω'}
    result = {}
    
    for term, count in counter.most_common(50):
        term_lower = term.lower()
        if count >= 2 and len(term) > 4 and not any(word in term_lower for word in common):
            result[term] = {"count": count, "translation": ""}
    
    return result

if __name__ == "__main__":
    if len(sys.argv) > 1:
        terms = extract_terms(sys.argv[1])
        with open('glossary.json', 'w', encoding='utf-8') as f:
            json.dump({"glossary": terms}, f, indent=2, ensure_ascii=False)
        print(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(terms)} —Ç–µ—Ä–º–∏–Ω–æ–≤")
PYEOF
    
    python3 extract_terms.py "$input_file"
    rm extract_terms.py
    
    log_info "‚úÖ –ì–ª–æ—Å—Å–∞—Ä–∏–π —Å–æ–∑–¥–∞–Ω: $glossary_file"
}

# –û—Å–Ω–æ–≤–Ω–æ–π –ø–µ—Ä–µ–≤–æ–¥
translate_document() {
    local input_file=$1
    local output_file="${input_file%.md}_translated.md"
    
    log_info "–ù–∞—á–∏–Ω–∞—é –ø–µ—Ä–µ–≤–æ–¥ $input_file..."
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫
    python3 deepseek_translator.py "$input_file" --output "$output_file" --glossary "glossary.json"
    
    if [ $? -eq 0 ]; then
        log_info "‚úÖ –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω: $output_file"
        
        # –°–æ–∑–¥–∞–µ–º –¥–≤—É—è–∑—ã—á–Ω—É—é –≤–µ—Ä—Å–∏—é
        log_info "–°–æ–∑–¥–∞—é –¥–≤—É—è–∑—ã—á–Ω—É—é –≤–µ—Ä—Å–∏—é..."
        python3 -c "
import sys
sys.path.append('.')
from deepseek_translator import DeepSeekTranslator
translator = DeepSeekTranslator()
translator.create_bilingual_file('$input_file', '$output_file')
        "
    else
        log_error "–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ"
        exit 1
    fi
}

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
check_quality() {
    local original=$1
    local translated=$2
    
    log_info "–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞..."
    
    cat > check_translation.py << 'PYEOF'
import sys

def check_basic_quality(orig_file, trans_file):
    with open(orig_file, 'r', encoding='utf-8') as f:
        orig = f.read()
    with open(trans_file, 'r', encoding='utf-8') as f:
        trans = f.read()
    
    # –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
    orig_lines = orig.count('\n')
    trans_lines = trans.count('\n')
    
    orig_words = len(orig.split())
    trans_words = len(trans.split())
    
    print(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª: {orig_lines} —Å—Ç—Ä–æ–∫, {orig_words} —Å–ª–æ–≤")
    print(f"–ü–µ—Ä–µ–≤–æ–¥: {trans_lines} —Å—Ç—Ä–æ–∫, {trans_words} —Å–ª–æ–≤")
    
    ratio = trans_words / orig_words if orig_words > 0 else 0
    print(f"–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å–ª–æ–≤: {ratio:.2f}")
    
    if 0.7 < ratio < 1.3:
        print("‚úÖ –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å–ª–æ–≤ –≤ –Ω–æ—Ä–º–µ")
    else:
        print("‚ö†Ô∏è  –í–æ–∑–º–æ–∂–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã —Å –ø–µ—Ä–µ–≤–æ–¥–æ–º")

if __name__ == "__main__":
    check_basic_quality(sys.argv[1], sys.argv[2])
PYEOF
    
    python3 check_translation.py "$original" "$translated"
    rm check_translation.py
}

# –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å
main() {
    echo "========================================"
    echo "   –ü–ï–†–ï–í–û–î–ß–ò–ö –¢–ï–•–ù–ò–ß–ï–°–ö–û–ô –î–û–ö–£–ú–ï–ù–¢–ê–¶–ò–ò   "
    echo "========================================"
    
    # –í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª
    INPUT_FILE="TechDocumFor-VergeLock.md"
    
    if [ ! -f "$INPUT_FILE" ]; then
        log_error "–§–∞–π–ª $INPUT_FILE –Ω–µ –Ω–∞–π–¥–µ–Ω"
        exit 1
    fi
    
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    check_dependencies
    
    # 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ API
    setup_api_key
    
    # 3. –°–æ–∑–¥–∞–Ω–∏–µ –≥–ª–æ—Å—Å–∞—Ä–∏—è
    create_glossary "$INPUT_FILE"
    
    # 4. –ü–µ—Ä–µ–≤–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    translate_document "$INPUT_FILE"
    
    # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    OUTPUT_FILE="${INPUT_FILE%.md}_translated.md"
    check_quality "$INPUT_FILE" "$OUTPUT_FILE"
    
    echo ""
    echo "========================================"
    echo "          üéâ –í–°–Å –ì–û–¢–û–í–û! ÔøΩÔøΩ             "
    echo "========================================"
    echo ""
    echo "–°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:"
    echo "  üìÑ –û—Ä–∏–≥–∏–Ω–∞–ª: $INPUT_FILE"
    echo "  üåê –ü–µ—Ä–µ–≤–æ–¥: $OUTPUT_FILE"
    echo "  üî§ –î–≤—É—è–∑—ã—á–Ω—ã–π: ${INPUT_FILE%.md}_bilingual.md"
    echo "  üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: ${INPUT_FILE%.md}_translated_stats.json"
    echo "  üìö –ì–ª–æ—Å—Å–∞—Ä–∏–π: glossary.json"
    echo ""
    echo "–î–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≥–ª–æ—Å—Å–∞—Ä–∏—è:"
    echo "  nano glossary.json"
    echo ""
    echo "–î–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞:"
    echo "  ./translate_project.sh"
}

# –ó–∞–ø—É—Å–∫
main "$@"
